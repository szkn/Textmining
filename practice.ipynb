{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキストマイニング第一回課題 ドキュメントの類似度を計算する  \n",
    "## ドキュメントの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: nltk in /Applications/anaconda3/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: click in /Applications/anaconda3/lib/python3.7/site-packages (from nltk) (7.1.1)\nRequirement already satisfied: joblib in /Applications/anaconda3/lib/python3.7/site-packages (from nltk) (0.14.1)\nRequirement already satisfied: regex in /Applications/anaconda3/lib/python3.7/site-packages (from nltk) (2020.6.8)\nRequirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.7/site-packages (from nltk) (4.45.0)\nRequirement already satisfied: gensim in /Applications/anaconda3/lib/python3.7/site-packages (3.8.3)\nRequirement already satisfied: scipy>=0.18.1 in /Applications/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\nRequirement already satisfied: numpy>=1.11.3 in /Applications/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\nRequirement already satisfied: smart-open>=1.8.1 in /Applications/anaconda3/lib/python3.7/site-packages (from gensim) (2.0.0)\nRequirement already satisfied: six>=1.5.0 in /Applications/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\nRequirement already satisfied: boto3 in /Applications/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.14.13)\nRequirement already satisfied: boto in /Applications/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\nRequirement already satisfied: requests in /Applications/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\nRequirement already satisfied: botocore<1.18.0,>=1.17.13 in /Applications/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.13)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Applications/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Applications/anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\nRequirement already satisfied: chardet<4,>=3.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\nRequirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Applications/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.13->boto3->smart-open>=1.8.1->gensim) (2.8.1)\nRequirement already satisfied: docutils<0.16,>=0.10 in /Applications/anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.13->boto3->smart-open>=1.8.1->gensim) (0.15.2)\nCollecting pandas\n  Downloading pandas-1.0.5-cp37-cp37m-macosx_10_9_x86_64.whl (10.0 MB)\n\u001b[K     |████████████████████████████████| 10.0 MB 3.1 MB/s \n\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: numpy>=1.13.3 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas) (1.18.1)\nRequirement already satisfied: pytz>=2017.2 in /Applications/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\nRequirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\nInstalling collected packages: pandas\nSuccessfully installed pandas-1.0.5\n"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to /Users/kento/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/kento/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/kento/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /Users/kento/nltk_data...\n[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# twitterのサンプルツイートを取得\n",
    "nltk.download(\"twitter_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n@97sides CONGRATS :)\nyeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n@BhaktisBanter @PallaviRuhail This one is irresistible :)\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM\nWe don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\nJgh , but we have to go to Bayan :D bye\nAs an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n\nWell… as the name implies :p.\n"
    }
   ],
   "source": [
    "# twiiterサンプルから10文抜き取って分析を行う。\n",
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "tweets = positive_tweets[:10]\n",
    "for i in range(10):\n",
    "    print(tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['    for being top engaged members in my community this week :)', ' Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', ' we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', ' CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '  This one is irresistible :)\\n ', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) \", ' On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n"
    }
   ],
   "source": [
    "import re\n",
    "def cleaning_text(text):\n",
    "    # @の削除\n",
    "    pattern1 = '@[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+'\n",
    "    text = re.sub(pattern1, '', text)\n",
    "    pattern2 = '#[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+'\n",
    "    text = re.sub(pattern2, '', text)\n",
    "    pattern3 = 'https://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+'\n",
    "    text = re.sub(pattern3, '', text)\n",
    "    pattern4 = 'http://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+'\n",
    "    text = re.sub(pattern4, '', text)\n",
    "    return text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "for being top engaged members in my community this week :)\n"
    }
   ],
   "source": [
    "print(clear_tweets[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# トークン化を施す\n",
    "def tokenize_text(text):\n",
    "  text = re.sub('[.,]', '', text)\n",
    "  return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['for', 'being', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n['hey', 'james!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks!']\n['we', 'have', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'be', 'an', 'amaze', 'track', 'when', 'are', 'you', 'in', 'scotland?!']\n['congrats', ':)']\n['yeaaaah', 'yippppy!!!', 'my', 'accnt', 'verify', 'rqst', 'ha', 'succeed', 'get', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n['this', 'one', 'be', 'irresistible', ':)']\n['we', \"don't\", 'like', 'to', 'keep', 'our', 'lovely', 'customer', 'waiting', 'for', 'long!', 'we', 'hope', 'you', 'enjoy!', 'happy', 'friday!', '-', 'lwwf', ':)']\n['on', 'second', 'thought', 'there’s', 'just', 'not', 'enough', 'time', 'for', 'a', 'dd', ':)', 'but', 'new', 'shorts', 'entering', 'system', 'sheep', 'must', 'be', 'buying']\n['jgh', 'but', 'we', 'have', 'to', 'go', 'to', 'bayan', ':d', 'bye']\n['as', 'an', 'act', 'of', 'mischievousness', 'am', 'calling', 'the', 'etl', 'layer', 'of', 'our', 'in-house', 'warehousing', 'app', 'katamari', 'well…', 'as', 'the', 'name', 'imply', ':p']\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def lemmatize_word(word):\n",
    "    # make words lower  example: Python =>python\n",
    "    word=word.lower()\n",
    "    \n",
    "    # lemmatize  example: cooked=>cook\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "      return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[None, None, 'top', 'engage', 'member', None, None, 'community', None, 'week', ':)']\n['hey', 'james!', None, 'odd', ':/', 'please', 'call', None, 'contact', 'centre', None, '02392441234', None, None, None, None, 'able', None, 'assist', None, ':)', 'many', 'thanks!']\n[None, None, None, 'listen', 'last', 'night', ':)', None, None, 'bleed', None, None, 'amaze', 'track', None, None, None, None, 'scotland?!']\n['congrats', ':)']\n['yeaaaah', 'yippppy!!!', None, 'accnt', 'verify', 'rqst', 'ha', 'succeed', 'get', None, 'blue', 'tick', 'mark', None, None, 'fb', 'profile', ':)', None, '15', 'days']\n[None, 'one', None, 'irresistible', ':)']\n[None, None, 'like', None, 'keep', None, 'lovely', 'customer', 'waiting', None, 'long!', None, 'hope', None, 'enjoy!', 'happy', 'friday!', '-', 'lwwf', ':)']\n[None, 'second', 'thought', 'there’s', None, None, 'enough', 'time', None, None, 'dd', ':)', None, 'new', 'shorts', 'entering', 'system', 'sheep', 'must', None, 'buying']\n['jgh', None, None, None, None, 'go', None, 'bayan', ':d', 'bye']\n[None, None, 'act', None, 'mischievousness', None, 'calling', None, 'etl', 'layer', None, None, 'in-house', 'warehousing', 'app', 'katamari', 'well…', None, None, 'name', 'imply', ':p']\n"
    }
   ],
   "source": [
    "#1 nltkのストップワードリスト\n",
    "en_stop = nltk.corpus.stopwords.words('english')\n",
    "# print(en_stop)\n",
    "def remove_stopwords(word, stopwordset):\n",
    "  if word in stopwordset:\n",
    "    return None\n",
    "  else:\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['top', 'engage', 'member', 'community', 'week', ':)'],\n ['hey',\n  'james!',\n  'odd',\n  ':/',\n  'please',\n  'call',\n  'contact',\n  'centre',\n  '02392441234',\n  'able',\n  'assist',\n  ':)',\n  'many',\n  'thanks!'],\n ['listen', 'last', 'night', ':)', 'bleed', 'amaze', 'track', 'scotland?!'],\n ['congrats', ':)'],\n ['yeaaaah',\n  'yippppy!!!',\n  'accnt',\n  'verify',\n  'rqst',\n  'ha',\n  'succeed',\n  'get',\n  'blue',\n  'tick',\n  'mark',\n  'fb',\n  'profile',\n  ':)',\n  '15',\n  'days'],\n ['one', 'irresistible', ':)'],\n ['like',\n  'keep',\n  'lovely',\n  'customer',\n  'waiting',\n  'long!',\n  'hope',\n  'enjoy!',\n  'happy',\n  'friday!',\n  '-',\n  'lwwf',\n  ':)'],\n ['second',\n  'thought',\n  'there’s',\n  'enough',\n  'time',\n  'dd',\n  ':)',\n  'new',\n  'shorts',\n  'entering',\n  'system',\n  'sheep',\n  'must',\n  'buying'],\n ['jgh', 'go', 'bayan', ':d', 'bye'],\n ['act',\n  'mischievousness',\n  'calling',\n  'etl',\n  'layer',\n  'in-house',\n  'warehousing',\n  'app',\n  'katamari',\n  'well…',\n  'name',\n  'imply',\n  ':p']]"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "def preprocessing_text(text):\n",
    "  text = cleaning_text(text)\n",
    "  tokens = tokenize_text(text)\n",
    "  tokens = [lemmatize_word(word) for word in tokens]\n",
    "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
    "  tokens = [word for word in tokens if word is not None]\n",
    "  return tokens\n",
    "\n",
    "preprocessed_docs = [preprocessing_text(text) for text in tweets]\n",
    "preprocessed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用した表現手法  \n",
    "今回はベクトル化の手法としてBoWと、TF-IDFを使用して解析を行った。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
    }
   ],
   "source": [
    "# BoWの実装\n",
    "def bow_vectorizer(docs):\n",
    "  word2id = {}\n",
    "  for doc in docs:\n",
    "    for w in doc:\n",
    "      if w not in word2id:\n",
    "        word2id[w] = len(word2id)\n",
    "        \n",
    "  result_list = []\n",
    "  for doc in docs:\n",
    "    doc_vec = [0] * len(word2id)\n",
    "    for w in doc:\n",
    "      doc_vec[word2id[w]] += 1\n",
    "    result_list.append(doc_vec)\n",
    "  return result_list, word2id\n",
    "\n",
    "bow_vec, word2id = bow_vectorizer(preprocessed_docs)\n",
    "print(bow_vec)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# BoWの要素を確認\n",
    "word2id.items()"
   ],
   "execution_count": 98,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_items([('top', 0), ('engage', 1), ('member', 2), ('community', 3), ('week', 4), (':)', 5), ('hey', 6), ('james!', 7), ('odd', 8), (':/', 9), ('please', 10), ('call', 11), ('contact', 12), ('centre', 13), ('02392441234', 14), ('able', 15), ('assist', 16), ('many', 17), ('thanks!', 18), ('listen', 19), ('last', 20), ('night', 21), ('bleed', 22), ('amaze', 23), ('track', 24), ('scotland?!', 25), ('congrats', 26), ('yeaaaah', 27), ('yippppy!!!', 28), ('accnt', 29), ('verify', 30), ('rqst', 31), ('ha', 32), ('succeed', 33), ('get', 34), ('blue', 35), ('tick', 36), ('mark', 37), ('fb', 38), ('profile', 39), ('15', 40), ('days', 41), ('one', 42), ('irresistible', 43), ('like', 44), ('keep', 45), ('lovely', 46), ('customer', 47), ('waiting', 48), ('long!', 49), ('hope', 50), ('enjoy!', 51), ('happy', 52), ('friday!', 53), ('-', 54), ('lwwf', 55), ('second', 56), ('thought', 57), ('there’s', 58), ('enough', 59), ('time', 60), ('dd', 61), ('new', 62), ('shorts', 63), ('entering', 64), ('system', 65), ('sheep', 66), ('must', 67), ('buying', 68), ('jgh', 69), ('go', 70), ('bayan', 71), (':d', 72), ('bye', 73), ('act', 74), ('mischievousness', 75), ('calling', 76), ('etl', 77), ('layer', 78), ('in-house', 79), ('warehousing', 80), ('app', 81), ('katamari', 82), ('well…', 83), ('name', 84), ('imply', 85), (':p', 86)])"
     },
     "metadata": {},
     "execution_count": 98
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFの実装\n",
    "def tfidf_vectorizer(docs):\n",
    "  def tf(word2id, doc):\n",
    "    term_counts = np.zeros(len(word2id))\n",
    "    for term in word2id.keys():\n",
    "      term_counts[word2id[term]] = doc.count(term)\n",
    "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
    "    return tf_values\n",
    "  \n",
    "  def idf(word2id, docs):\n",
    "    idf = np.zeros(len(word2id))\n",
    "    for term in word2id.keys():\n",
    "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
    "    return idf\n",
    "  \n",
    "  word2id = {}\n",
    "  for doc in docs:\n",
    "    for w in doc:\n",
    "      if w not in word2id:\n",
    "        word2id[w] = len(word2id)\n",
    "  \n",
    "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.3837641821656743, 0.3837641821656743, 0.3837641821656743, 0.3837641821656743, 0.3837641821656743, 0.037190591885701625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.015938825093872126, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.02789294391427622, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28782313662425574, 0.28782313662425574, 0.28782313662425574, 0.28782313662425574, 0.28782313662425574, 0.28782313662425574, 0.28782313662425574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.11157177565710488, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.151292546497023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.01394647195713811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.07438118377140325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7675283643313486, 0.7675283643313486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.01716488856263152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.015938825093872126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4605170185988092, 0.4605170185988092, 0.4605170185988092, 0.4605170185988092, 0.4605170185988092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123]]\ndict_items([('top', 0), ('engage', 1), ('member', 2), ('community', 3), ('week', 4), (':)', 5), ('hey', 6), ('james!', 7), ('odd', 8), (':/', 9), ('please', 10), ('call', 11), ('contact', 12), ('centre', 13), ('02392441234', 14), ('able', 15), ('assist', 16), ('many', 17), ('thanks!', 18), ('listen', 19), ('last', 20), ('night', 21), ('bleed', 22), ('amaze', 23), ('track', 24), ('scotland?!', 25), ('congrats', 26), ('yeaaaah', 27), ('yippppy!!!', 28), ('accnt', 29), ('verify', 30), ('rqst', 31), ('ha', 32), ('succeed', 33), ('get', 34), ('blue', 35), ('tick', 36), ('mark', 37), ('fb', 38), ('profile', 39), ('15', 40), ('days', 41), ('one', 42), ('irresistible', 43), ('like', 44), ('keep', 45), ('lovely', 46), ('customer', 47), ('waiting', 48), ('long!', 49), ('hope', 50), ('enjoy!', 51), ('happy', 52), ('friday!', 53), ('-', 54), ('lwwf', 55), ('second', 56), ('thought', 57), ('there’s', 58), ('enough', 59), ('time', 60), ('dd', 61), ('new', 62), ('shorts', 63), ('entering', 64), ('system', 65), ('sheep', 66), ('must', 67), ('buying', 68), ('jgh', 69), ('go', 70), ('bayan', 71), (':d', 72), ('bye', 73), ('act', 74), ('mischievousness', 75), ('calling', 76), ('etl', 77), ('layer', 78), ('in-house', 79), ('warehousing', 80), ('app', 81), ('katamari', 82), ('well…', 83), ('name', 84), ('imply', 85), (':p', 86)])\n"
    }
   ],
   "source": [
    "tfidf_vector, word2id = tfidf_vectorizer(preprocessed_docs)\n",
    "print(tfidf_vector)\n",
    "print(word2id.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}